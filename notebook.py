# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bDQl924b7lj1ggHNYQQrys78TOsV-lol

# Proyek Klasifikasi Gambar: Klasifikasi Gambar Hewan (Animals-10)
- **Nama:** Khatama Putra
- **Email:** m297d5y0968@student.devacademy.id
- **ID Dicoding:** khatamap
"""

!pip install pipreqs
!pipreqs "/content/drive/MyDrive/Colab Notebooks/BFDL_Khatama-Putra_CNN" --scan-notebooks

"""## Import Semua Packages/Library yang Digunakan"""

!pip install -q kaggle
!pip install -q split-folders
!pip install -q tensorflowjs

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.regularizers import l2
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Input
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

from google.colab import files
from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
import os
import zipfile
import shutil
import splitfolders
import pathlib
import subprocess

"""## Data Preparation

### Data Loading
"""

from google.colab import drive
drive.mount('/content/drive')

# Membuat direktori .kaggle dan memindahkan file kaggle.json
!mkdir -p ~/.kaggle
!cp "/content/drive/MyDrive/Colab Notebooks/BFDL_Khatama-Putra_CNN/kaggle.json" ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

print("Setup Kaggle API selesai.")

!kaggle datasets download viratkothari/animal10

print("Dataset berhasil diunduh.")

"""### Data Preprocessing

#### Split Dataset
"""

zip_path = 'animal10.zip'
output_dir = 'animals_split'

if os.path.exists(output_dir): shutil.rmtree(output_dir)
if os.path.exists('Animals-10'): shutil.rmtree('Animals-10')

# Ekstrak
print("Mengekstrak dataset...")
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall()

# Cari folder hasil ekstrak
base_dir = None
for item in os.listdir('.'):
    if os.path.isdir(item) and item not in ['.config', 'sample_data', '.kaggle', output_dir]:
        if len(os.listdir(item)) > 5:
            base_dir = item
            break

if base_dir:
    print(f"Dataset ditemukan di folder: '{base_dir}'")
    print(f"Kelas: {os.listdir(base_dir)}")

    # Split Data
    print(f"Membagi dataset ke '{output_dir}'...")
    splitfolders.ratio(base_dir, output=output_dir, seed=42, ratio=(.8, .1, .1))

    # Path
    train_dir = os.path.join(output_dir, 'train')
    val_dir = os.path.join(output_dir, 'val')
    test_dir = os.path.join(output_dir, 'test')

    print("Setup Dataset Selesai!")
else:
    print("Error: Folder dataset tidak ditemukan setelah ekstrak.")

"""#### Image Augmentation"""

TARGET_SIZE = (224, 224)
BATCH_SIZE = 64

# Pakai preprocess_input bawaan MobileNetV2
train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_test_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input
)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=TARGET_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

validation_generator = val_test_datagen.flow_from_directory(
    val_dir,
    target_size=TARGET_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

test_generator = val_test_datagen.flow_from_directory(
    test_dir,
    target_size=TARGET_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

class_names = list(train_generator.class_indices.keys())
print(f"Label Kelas: {class_names}")

"""## Modelling"""

# Base Model (Transfer Learning)
base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))
base_model.trainable = False

model = Sequential([
    base_model,

    Conv2D(128, (3, 3), activation='relu', padding='same'),

    GlobalAveragePooling2D(),
    Dropout(0.2),

    Dense(256, activation='relu'),
    Dropout(0.2),

    Dense(len(class_names), activation='softmax')
])

model.compile(
    loss='categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    metrics=['accuracy']
)

model.summary()

callbacks = [
    EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, restore_best_weights=True),
    ModelCheckpoint('best_animal_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)
]

EPOCHS = 20

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=callbacks,
    verbose=1
)

"""## Evaluasi dan Visualisasi"""

# Plot Akurasi
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.tight_layout()
plt.show()

# Evaluasi model pada test set
print("Mengevaluasi model pada Test Set...")
test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)

print(f"\nTest Loss: {test_loss * 100:.2f}%")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

"""## Konversi Model"""

output_save_path = "/content"

# SavedModel
saved_model_dir = 'animals_model_savedmodel'
saved_model_path = os.path.join(output_save_path, saved_model_dir)

model.export(saved_model_path)
print(f"âœ… SavedModel berhasil diexport ke: {saved_model_path}")

zip_saved_model = os.path.join(output_save_path, saved_model_dir)
shutil.make_archive(zip_saved_model, 'zip', saved_model_path)

# TF-Lite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

tflite_model_path = os.path.join(output_save_path, 'animals_model.tflite')
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)

# TFJS
!pip install -q tensorflowjs
tfjs_dir = 'animals_model_tfjs'
tfjs_model_path = os.path.join(output_save_path, tfjs_dir)

if os.path.exists(tfjs_model_path):
    shutil.rmtree(tfjs_model_path)

command = f"tensorflowjs_converter --input_format=tf_saved_model {saved_model_path} {tfjs_model_path}"
subprocess.run(command, shell=True, check=True)

zip_tfjs = os.path.join(output_save_path, tfjs_dir)
shutil.make_archive(zip_tfjs, 'zip', tfjs_model_path)

# Download
files.download(tflite_model_path)
files.download(f"{zip_saved_model}.zip")
files.download(f"{zip_tfjs}.zip")

"""## Inference (Optional)"""

# Fungsi untuk memuat, memproses, dan memprediksi gambar
def predict_uploaded_image(model_to_use, class_names_list):
    uploaded = files.upload()

    for fn in uploaded.keys():
        path = fn

        img = image.load_img(path, target_size=TARGET_SIZE)

        plt.imshow(img)
        plt.axis('off')
        plt.show()

        img_array = image.img_to_array(img)

        img_array /= 255.0

        img_array = np.expand_dims(img_array, axis=0)

        predictions = model_to_use.predict(img_array)

        prediction_tensor = predictions['output_0'] if isinstance(predictions, dict) else predictions

        predicted_class_index = np.argmax(prediction_tensor[0])
        predicted_class_name = class_names_list[predicted_class_index]
        confidence = np.max(prediction_tensor[0]) * 100

        print(f"File: {fn}")
        print(f"Hasil Prediksi: {predicted_class_name}")
        print(f"Tingkat Keyakinan: {confidence:.2f}%")

tfsmlayer = tf.keras.layers.TFSMLayer(saved_model_path, call_endpoint='serving_default')
loaded_model = tf.keras.Sequential([tfsmlayer])

predict_uploaded_image(loaded_model, class_names)

predict_uploaded_image(loaded_model, class_names)

predict_uploaded_image(loaded_model, class_names)